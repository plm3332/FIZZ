{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors](https://arxiv.org/abs/2205.12854)\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "@misc{tang2023understanding,\n",
    "      title={Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors}, \n",
    "      author={Liyan Tang and Tanya Goyal and Alexander R. Fabbri and Philippe Laban and Jiacheng Xu and Semih Yavuz and Wojciech Kryściński and Justin F. Rousseau and Greg Durrett},\n",
    "      year={2023},\n",
    "      eprint={2205.12854},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.CL}\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from utils import choose_best_threshold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import SOTA, XFORMER, OLD, MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path=\"data/aggre_fact_sota.csv\"\n",
    "# dataset_path=\"data/aggre_fact_sota_granularity.csv\"\n",
    "df = pd.read_csv(dataset_path, index_col = 0)\n",
    "\n",
    "# split data\n",
    "df_val = df[df.cut == 'val']\n",
    "df_val_sota = df_val[df_val.model_name.isin(SOTA)]\n",
    "df_test = df[df.cut == 'test']\n",
    "df_test_sota = df_test[df_test.model_name.isin(SOTA)]\n",
    "\n",
    "dataset_list = ['XSumFaith', 'Polytope', 'FactCC', 'SummEval', 'FRANK', 'Wang20', 'CLIFF', 'Goyal21', 'Cao22']\n",
    "systems = ['DAE', 'QuestEval', 'SummaC-ZS', 'SummaC-Conv', 'QAFactEval', 'FaCED', 'FaCED_wo_GE', 'AlignScore']\n",
    "# systems = ['DAE', 'QuestEval', 'SummaC-ZS', 'SummaC-Conv', 'QAFactEval', 'FaCED-1G', 'FaCED-2G', 'FaCED-3G', 'FaCED-4G']\n",
    "origins = ['cnndm', 'xsum']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AggreFact-CNN/XSum-SOTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import resample_balanced_acc\n",
    "\n",
    "main_sota_df = pd.DataFrame(\n",
    "    columns=['system', 'origin', 'bl_acc']\n",
    ")\n",
    "\n",
    "results = []\n",
    "\n",
    "for system in systems:\n",
    "    for origin in origins:\n",
    "        df_val_temp = df_val_sota[(df_val_sota.origin == origin)]\n",
    "        df_test_temp = df_test_sota[(df_test_sota.origin == origin)]\n",
    "\n",
    "        best_thresh, best_f1 = choose_best_threshold(df_val_temp.label.values, df_val_temp[f'{system}_score'].values)\n",
    "        scores_test = df_test_temp[f'{system}_score'].values\n",
    "        preds_test = [1 if score > best_thresh else 0 for score in scores_test]\n",
    "\n",
    "        f1_score = sklearn.metrics.balanced_accuracy_score(df_test_temp.label.values, preds_test)\n",
    "\n",
    "        main_sota_df.loc[len(main_sota_df.index)] = [\n",
    "            system, origin, f1_score\n",
    "        ]\n",
    "\n",
    "        results.append({\"system\": system, 'origin': origin,  \"labels\": df_test_temp.label.values, \n",
    "        \"preds\": preds_test, \"scores\": scores_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnndm          DAE           - 0.654, 0.034\n",
      "cnndm       QuestEval        - 0.702, 0.030\n",
      "cnndm       SummaC-ZS        - 0.640, 0.033\n",
      "cnndm      SummaC-Conv       - 0.610, 0.032\n",
      "cnndm       QAFactEval       - 0.678, 0.033\n",
      "cnndm         FaCED          - 0.726, 0.029\n",
      "cnndm      FaCED_wo_GE       - 0.722, 0.029\n",
      "cnndm       AlignScore       - 0.625, 0.033\n",
      "\n",
      " xsum          DAE           - 0.702, 0.019\n",
      " xsum       QuestEval        - 0.595, 0.021\n",
      " xsum       SummaC-ZS        - 0.564, 0.014\n",
      " xsum      SummaC-Conv       - 0.650, 0.018\n",
      " xsum       QAFactEval       - 0.639, 0.020\n",
      " xsum         FaCED          - 0.693, 0.020\n",
      " xsum      FaCED_wo_GE       - 0.663, 0.019\n",
      " xsum       AlignScore       - 0.696, 0.017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Table 5\n",
    "# standard deviation may differ due to randomness\n",
    "\n",
    "# from https://github.com/tingofurro/summac/\n",
    "P5 = 5 / 2 # Correction due to the fact that we are running 2 tests with the same data\n",
    "P1 = 1 / 2 # Correction due to the fact that we are running 2 tests with the same data\n",
    "\n",
    "for origin in origins:\n",
    "    sampled_batch_preds = {res[\"system\"]: [] for res in results}\n",
    "    \n",
    "    for res in results:\n",
    "        if res['origin'] == origin:\n",
    "    \n",
    "            samples = resample_balanced_acc(res[\"preds\"], res[\"labels\"])\n",
    "            sampled_batch_preds[res[\"system\"]].append(samples)\n",
    "            low5, high5 = np.percentile(samples, P5), np.percentile(samples, 100-P5)\n",
    "            low1, high1 = np.percentile(samples, P1), np.percentile(samples, 100-P1)\n",
    "            bacc = sklearn.metrics.balanced_accuracy_score(res[\"labels\"], res[\"preds\"])\n",
    "\n",
    "            print(res['origin'].center(6), res[\"system\"].center(20), \" - %.3f, %.3f\" % (bacc, bacc-low5))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8b59c53c64597fe44953bbb80381bd95288c336271f58402d1e3936b61a22f28"
  },
  "kernelspec": {
   "display_name": "summac",
   "language": "python",
   "name": "summac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
